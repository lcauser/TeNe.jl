<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tensors · TeNe.jl</title><meta name="title" content="Tensors · TeNe.jl"/><meta property="og:title" content="Tensors · TeNe.jl"/><meta property="twitter:title" content="Tensors · TeNe.jl"/><meta name="description" content="Documentation for TeNe.jl."/><meta property="og:description" content="Documentation for TeNe.jl."/><meta property="twitter:description" content="Documentation for TeNe.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">TeNe.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>Tensors</a><ul class="internal"><li><a class="tocitem" href="#Tensor-operations"><span>Tensor operations</span></a></li><li><a class="tocitem" href="#Tensor-factorisations"><span>Tensor factorisations</span></a></li></ul></li><li><a class="tocitem" href="../mps/">Matrix product states</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Tensors</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tensors</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/lcauser/TeNe.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/lcauser/TeNe.jl/blob/main/docs/src/tensors.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Tensors"><a class="docs-heading-anchor" href="#Tensors">Tensors</a><a id="Tensors-1"></a><a class="docs-heading-anchor-permalink" href="#Tensors" title="Permalink"></a></h1><ul><li><a href="#Tensors">Tensors</a></li><li class="no-marker"><ul><li><a href="#Tensor-operations">Tensor operations</a></li><li class="no-marker"><ul><li><a href="#Tensor-contraction">Tensor contraction</a></li><li><a href="#Tensor-product">Tensor product</a></li><li><a href="#Tensor-trace">Tensor trace</a></li><li><a href="#Permuting-a-single-dimension">Permuting a single dimension</a></li><li><a href="#Combining-and-restoring-dimensions">Combining &amp; restoring dimensions</a></li></ul></li><li><a href="#Tensor-factorisations">Tensor factorisations</a></li><li class="no-marker"><ul><li><a href="#Singular-value-decomposition">Singular value decomposition</a></li></ul></li></ul></li></ul><h2 id="Tensor-operations"><a class="docs-heading-anchor" href="#Tensor-operations">Tensor operations</a><a id="Tensor-operations-1"></a><a class="docs-heading-anchor-permalink" href="#Tensor-operations" title="Permalink"></a></h2><h3 id="Tensor-contraction"><a class="docs-heading-anchor" href="#Tensor-contraction">Tensor contraction</a><a id="Tensor-contraction-1"></a><a class="docs-heading-anchor-permalink" href="#Tensor-contraction" title="Permalink"></a></h3><p>Contract two tensors <code>x</code> and <code>y</code> over dimensions <code>cix</code> and <code>ciy</code> with a shared size. This can be done in place with a pre-allocated tensor <code>z</code> using <code>contract!(z, x, y, cix, ciy)</code>, or otherwise <code>contract(x, y, cix, ciy)</code>. The dimensions <code>cix</code> and <code>ciy</code> can be specified as integers or tuples of integers for contractions over multiple dimensions. Optionally, the tensors used in the contraction can be conjugated using the arguments <code>conjx</code> and <code>conjy</code>. Note that, by default, the result will be stored in the memory cache (using keyword argument <code>tocache</code>). If the contraction is not some intermediate step, and you would like to save the resulting tensor for future use, then use <code>tocache=false</code>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TeNe.contract!" href="#TeNe.contract!"><code>TeNe.contract!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">contract!(z, x, y, cix, ciy, [conjx=false, conjy=false])</code></pre><p>Contract tensors <code>x</code> and <code>y</code> across dimensions <code>cix</code> and <code>ciy</code>, and store the result in <code>z</code>. In-place version of contract.</p><p><strong>Arguments</strong></p><pre><code class="nohighlight hljs">- `z`: tensor to store the result.
- `x`: first tensor to contract.
- `y&#39;: second tensor to contract.
- `cix`: the dimensions of the first tensor to contract.
- `ciy`: the dimensions of the second tensor to contract.
- `conjx::Bool=false`: Take the complex conjugate of argument x?
- `conjy::Bool=false`: Take the complex conjugate of argument y?</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; x = randn(ComplexF64, 2, 3, 4);
julia&gt; y = randn(ComplexF64, 3, 5, 6);
julia&gt; z = similar(x, (2, 4, 5, 6));
julia&gt; contract!(z, x, y, 2, 1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/lcauser/TeNe.jl/blob/334bfb9b6f3b86c498a300423853f10c6142070a/src/tensors/contract.jl#L67-L91">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TeNe.contract" href="#TeNe.contract"><code>TeNe.contract</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">contract(x, y, cix, ciy, [conjx=false, conjy=false]; kwargs...)</code></pre><p>Contract tensors <code>x</code> and <code>y</code> across dimensions <code>cix</code> and <code>ciy</code>, and returns it as <code>z</code>.</p><p><strong>Arguments</strong></p><pre><code class="nohighlight hljs">- `x`: first tensor to contract.
- `y&#39;: second tensor to contract.
- `cix`: the dimensions of the first tensor to contract.
- `ciy`: the dimensions of the second tensor to contract.
- `conjx::Bool=false`: Take the complex conjugate of argument x?
- `conjy::Bool=false`: Take the complex conjugate of argument y?</code></pre><p><strong>Optional Keyword Arguments</strong></p><pre><code class="nohighlight hljs">- `tocache::Bool=true`: store the result in the second level of the cache?
- `sublevel=:auto`: if stored in cache, at which sublevel? :auto finds non-aliased memory</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; x = randn(ComplexF64, 2, 3, 4);
julia&gt; y = randn(ComplexF64, 3, 5, 6);
julia&gt; z = contract(x, y, 2, 1);
julia&gt; size(z)
(2, 4, 5, 6)</code></pre><pre><code class="language-julia-repl hljs">julia&gt; x = randn(ComplexF64, 2, 3, 4, 5);
julia&gt; y = randn(ComplexF64, 6, 5, 2, 7);
julia&gt; z = contract(x, y, (1, 4), (3, 2));
julia&gt; size(z)
(3, 4, 6, 7)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/lcauser/TeNe.jl/blob/334bfb9b6f3b86c498a300423853f10c6142070a/src/tensors/contract.jl#L9-L45">source</a></section></article><h3 id="Tensor-product"><a class="docs-heading-anchor" href="#Tensor-product">Tensor product</a><a id="Tensor-product-1"></a><a class="docs-heading-anchor-permalink" href="#Tensor-product" title="Permalink"></a></h3><p>Take the tensor product over two tensors <code>x</code> and <code>y</code> to give a single tensor. This can be done in place with a pre-allocated tensor <code>z</code> using <code>tensorproduct!(z, x, y)</code>, or otherwise <code>tensorproduct(x, y)</code>. Optionally, the tensors used in the product can be conjugated using the arguments <code>conjx</code> and <code>conjy</code>. Note that, by default, the result will be stored in the memory cache (using keyword argument <code>tocache</code>). If the result is not some intermediate step, and you would like to save the resulting tensor for future use, then use <code>tocache=false</code>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TeNe.tensorproduct!" href="#TeNe.tensorproduct!"><code>TeNe.tensorproduct!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">tensorproduct!(z, x, y, [conjx=false, conjy=false])</code></pre><p>Compute the tensor product of the two tensors <code>x</code> and <code>y</code>, and store the  result in <code>z</code>. Optionally, do the tensor product using the conjugate of the tensors.</p><p><strong>Arguments</strong></p><pre><code class="nohighlight hljs">- `z`: tensor to store the result.
- `x`: first tensor.
- `y&#39;: second tensor.
- `conjx::Bool=false`: Take the complex conjugate of argument x?
- `conjy::Bool=false`: Take the complex conjugate of argument y?</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; x = randn(ComplexF64, 2, 3);
julia&gt; y = randn(ComplexF64, 4, 5);
julia&gt; z = similar(x, (2, 3, 4, 5));
julia&gt; tensorproduct!(z, x, y);</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/lcauser/TeNe.jl/blob/334bfb9b6f3b86c498a300423853f10c6142070a/src/tensors/tensorproduct.jl#L9-L32">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TeNe.tensorproduct" href="#TeNe.tensorproduct"><code>TeNe.tensorproduct</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">tensorproduct(x, y, [conjx=false, conjy=false])</code></pre><p>Compute the tensor product of the two tensors <code>x</code> and <code>y</code>, and store the  result in <code>z</code>. Optionally, do the tensor product using the conjugate of the tensors.</p><p><strong>Arguments</strong></p><pre><code class="nohighlight hljs">- `x`: first tensor.
- `y&#39;: second tensor.
- `conjx::Bool=false`: Take the complex conjugate of argument x?
- `conjy::Bool=false`: Take the complex conjugate of argument y?</code></pre><p><strong>Optional Keyword Arguments</strong></p><pre><code class="nohighlight hljs">- `tocache::Bool=true`: store the result in the second level of the cache?
- `sublevel=:auto`: if stored in cache, at which sublevel? :auto finds non-aliased memory</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; x = randn(ComplexF64, 2, 3);
julia&gt; y = randn(ComplexF64, 4, 5);
julia&gt; z = tensorproduct(x, y);
julia&gt; size(z)
(2, 3, 4, 5)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/lcauser/TeNe.jl/blob/334bfb9b6f3b86c498a300423853f10c6142070a/src/tensors/tensorproduct.jl#L40-L68">source</a></section></article><h3 id="Tensor-trace"><a class="docs-heading-anchor" href="#Tensor-trace">Tensor trace</a><a id="Tensor-trace-1"></a><a class="docs-heading-anchor-permalink" href="#Tensor-trace" title="Permalink"></a></h3><p>Compute the trace over multiple dimensions <code>cix</code> in a tensor <code>x</code>. This can be done in place with a pre-allocated tensor <code>z</code> using <code>trace!(z, x, cix...)</code>, or otherwise <code>trace(x, cix...)</code>. Optionally, the tensor used in the trace can be conjugated using the key word argument <code>conj</code>. Note that, by default, the result will be stored in the memory cache (using keyword argument <code>tocache</code>). If the result is not some intermediate step, and you would like to save the resulting tensor for future use, then use <code>tocache=false</code>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TeNe.trace!" href="#TeNe.trace!"><code>TeNe.trace!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">trace!(z, x, cix::Int...; kwargs)</code></pre><p>Compute the trace of <code>x</code><code>over dimensions</code>cix<code>, and store the result in</code>z`. In place version of trace.</p><p><strong>Optional Keyword Arguments</strong></p><pre><code class="nohighlight hljs">- &#39;conj::Bool=false&#39;: take the conjugate?</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; x = randn(ComplexF64, 2, 3, 4, 3);
julia&gt; z = similar(x, (2, 4));
julia&gt; trace!(z, x, (2, 4));</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/lcauser/TeNe.jl/blob/334bfb9b6f3b86c498a300423853f10c6142070a/src/tensors/trace.jl#L9-L26">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TeNe.trace-Tuple{Any, Vararg{Int64, N} where N}" href="#TeNe.trace-Tuple{Any, Vararg{Int64, N} where N}"><code>TeNe.trace</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">trace(x, cix::Int...; kwargs)</code></pre><p>Compute the trace of <code>x</code> over dimensions <code>cix</code>.</p><p><strong>Optional Keyword Arguments</strong></p><pre><code class="nohighlight hljs">- `conj::Bool=false`: take the conjugate?
- `tocache::Bool=true`: store the result in the second level of the cache?
- `sublevel=:auto`: if stored in cache, at which sublevel? :auto finds non-aliased memory</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; x = randn(ComplexF64, 2, 3, 4, 3);
julia&gt; y = trace(x, 2, 4);
julia&gt; size(y)
(2, 4)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/lcauser/TeNe.jl/blob/334bfb9b6f3b86c498a300423853f10c6142070a/src/tensors/trace.jl#L37-L56">source</a></section></article><h3 id="Permuting-a-single-dimension"><a class="docs-heading-anchor" href="#Permuting-a-single-dimension">Permuting a single dimension</a><a id="Permuting-a-single-dimension-1"></a><a class="docs-heading-anchor-permalink" href="#Permuting-a-single-dimension" title="Permalink"></a></h3><p>Permute the dimension at position <code>i</code> in tensor <code>x</code> to position <code>j</code>. This can be done in place with a pre-allocated tensor <code>z</code> using <code>permutedim!(z, x, i, j)</code>, or otherwise <code>permutedim(x, i, j)</code>. Note that, by default, the result will be stored in the memory cache (using keyword argument <code>tocache</code>). If the result is not some intermediate step, and you would like to save the resulting tensor for future use, then use <code>tocache=false</code>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TeNe.permutedim!" href="#TeNe.permutedim!"><code>TeNe.permutedim!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">permutedim!(z, x, i, j; kwargs...)</code></pre><p>Permute dimension <code>i</code> to <code>j</code> for tensor <code>x</code>. Store the result in <code>z</code>. In place version of permutedim.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; x = randn(ComplexF64, 2, 3, 4, 5);
julia&gt; z = similar(x, (2, 4, 5, 3));
julia&gt; permutedims!(z, x, 2, 4);</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/lcauser/TeNe.jl/blob/334bfb9b6f3b86c498a300423853f10c6142070a/src/tensors/permutedim.jl#L41-L54">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TeNe.permutedim" href="#TeNe.permutedim"><code>TeNe.permutedim</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">permutedim(x, i::Int, j::Int; kwargs...)</code></pre><p>Permute dimension with position <code>i</code> to position <code>j</code> for tensor <code>x</code>.</p><p><strong>Optional Keyword Arguments</strong></p><pre><code class="nohighlight hljs">- `tocache::Bool=true`: store the result in the second level of the cache?
- `sublevel=:auto`: if stored in cache, at which sublevel? :auto finds non-aliased memory</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; x = randn(ComplexF64, 2, 3, 4, 5);
julia&gt; x = permutedim(x, 2, 4);
julia&gt; size(x)
(2, 4, 5, 3)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/lcauser/TeNe.jl/blob/334bfb9b6f3b86c498a300423853f10c6142070a/src/tensors/permutedim.jl#L7-L25">source</a></section></article><h3 id="Combining-and-restoring-dimensions"><a class="docs-heading-anchor" href="#Combining-and-restoring-dimensions">Combining &amp; restoring dimensions</a><a id="Combining-and-restoring-dimensions-1"></a><a class="docs-heading-anchor-permalink" href="#Combining-and-restoring-dimensions" title="Permalink"></a></h3><p>Dimensions in a tensor can be combined into a single dimension, and restored using a key. This allows us to make efficient use of BLAS and LAPACK routines involving matrix operations. </p><p>Combine the dimensions <code>cixs</code> of tensor <code>x</code>. This can be done in place with a pre-allocated tensor <code>z</code> using <code>key = combinedims!(z, x, cixs)</code>, or otherwise <code>z, key = combinedims(x, cixs)</code>. Note that, by default, the result will be stored in the memory cache (using keyword argument <code>tocache</code>). If the result is not some intermediate step, and you would like to save the resulting tensor for future use, then use <code>tocache=false</code>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TeNe.combinedims!" href="#TeNe.combinedims!"><code>TeNe.combinedims!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">combinedims!(y, x, cixs)</code></pre><p>Combine the dimensions cixs in tensor <code>x</code>, and store the result in <code>y</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; x = randn(ComplexF64, 4, 5, 6, 7);
julia&gt; y = similar(x, (4, 7, 30));
julia&gt; key = combinedims!(y, x, (2, 3))
((2, 3), (5, 6))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/lcauser/TeNe.jl/blob/334bfb9b6f3b86c498a300423853f10c6142070a/src/tensors/combinedims.jl#L63-L76">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TeNe.combinedims" href="#TeNe.combinedims"><code>TeNe.combinedims</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">combinedims(x, cixs; kwargs...)</code></pre><p>Combine the dimensions <code>cixs</code> in tensor <code>x</code>. </p><p>Returns the reshaped tensor, along with a <code>key</code> to restore the original permutations.</p><p><strong>Optional Keyword Arguments</strong></p><pre><code class="nohighlight hljs">- `tocache::Bool=true`: store the result in the second level of the cache?
- `sublevel=:auto`: which sublevel to store in the cache?
- `return_copy=false`: Return the result in newly allocated memory from the cache?
  Only necessary if the combined dimensions are the last dimensions of `x`.</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; x = randn(ComplexF64, 4, 5, 6, 7);
julia&gt; y, key = combinedims(x, (2, 3));
julia&gt; size(y)
(4, 7, 30)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/lcauser/TeNe.jl/blob/334bfb9b6f3b86c498a300423853f10c6142070a/src/tensors/combinedims.jl#L10-L32">source</a></section></article><p>After combining the dimensions, which returns a key, the dimensions of the tensor can be restored. This can be done in place with a pre-allocated tensor <code>y</code> using <code>uncombinedims!(y, x, key)</code>, or otherwise <code>y = uncombinedims(y, x, key)</code>. Note that, by default, the result will be stored in the memory cache (using keyword argument <code>tocache</code>). If the result is not some intermediate step, and you would like to save the resulting tensor for future use, then use <code>tocache=false</code>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TeNe.uncombinedims!" href="#TeNe.uncombinedims!"><code>TeNe.uncombinedims!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">uncombinedims!(y, x, key)</code></pre><p>Uncombine the end dimensions of <code>x</code> according to the key, and store the result in <code>y</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; x = randn(ComplexF64, 4, 5, 6, 7);
julia&gt; z, key = combinedims(x, (2, 3));
julia&gt; y = similar(x);
julia&gt; isapprox(y, x);
true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/lcauser/TeNe.jl/blob/334bfb9b6f3b86c498a300423853f10c6142070a/src/tensors/combinedims.jl#L179-L193">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TeNe.uncombinedims" href="#TeNe.uncombinedims"><code>TeNe.uncombinedims</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">uncombinedims(x, key; kwargs...)</code></pre><p>Uncombine the end dimension in tensor <code>x</code> according to the <code>key</code>.</p><p><strong>Key arguments</strong></p><pre><code class="nohighlight hljs">- `tocache::Bool=true`: store the result in the second level of the cache?
- `sublevel=:auto`: which sublevel to store in the cache?
- `return_copy=false`: Return the result in newly allocated memory from the cache?
  Only necessary if the combined dimensions are the last dimensions of `x`.</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; x = randn(ComplexF64, 4, 5, 6, 7);
julia&gt; y, key = combinedims(x, (2, 3));
julia&gt; z = uncombinedims(y, key);
julia&gt; size(z)
(4, 5, 6, 7)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/lcauser/TeNe.jl/blob/334bfb9b6f3b86c498a300423853f10c6142070a/src/tensors/combinedims.jl#L130-L151">source</a></section></article><h2 id="Tensor-factorisations"><a class="docs-heading-anchor" href="#Tensor-factorisations">Tensor factorisations</a><a id="Tensor-factorisations-1"></a><a class="docs-heading-anchor-permalink" href="#Tensor-factorisations" title="Permalink"></a></h2><h3 id="Singular-value-decomposition"><a class="docs-heading-anchor" href="#Singular-value-decomposition">Singular value decomposition</a><a id="Singular-value-decomposition-1"></a><a class="docs-heading-anchor-permalink" href="#Singular-value-decomposition" title="Permalink"></a></h3><p>The singular value decomposition <span>$M = USV$</span> is typically applied to a matrix, but can equally be applied to a tensor to split the dimensions into separate tensors. This is done by permuting and reshaping the tensor into a matrix representation and applying the SVD. The dimensions to be contained in <span>$V$</span> are specified by <code>dims</code>, with <code>U, S, V = tsvd(x, dims)</code>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TeNe.tsvd" href="#TeNe.tsvd"><code>TeNe.tsvd</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">tsvd(x, dims; kwargs...)
tsvd(x, dim::Int; kwargs...)</code></pre><p>Computer a singular value decomposition of tensor <code>x</code>. Seperates the dimensions <code>dims</code> from the remainder.</p><p><strong>Optional Keyword Arguments</strong></p><pre><code class="nohighlight hljs">- `cutoff::Float64=0.0`: Truncation criteria to reduce the bond dimension.
  Good values range from 1e-8 to 1e-14.
- `mindim::Int=1`: Mininum dimension for truncated.
- `maxdim::Int=0`: Maximum bond dimension for truncation. Set to 0 to have
  no limit.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/lcauser/TeNe.jl/blob/334bfb9b6f3b86c498a300423853f10c6142070a/src/tensors/svd.jl#L11-L26">source</a></section></article><p>At a later date, we would like to improve the SVD to pre-allocate memory in the cache for calculating the returns (and optional parameters to pre-allocated the memory to restore the results.)</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><a class="docs-footer-nextpage" href="../mps/">Matrix product states »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.0 on <span class="colophon-date" title="Thursday 25 April 2024 10:06">Thursday 25 April 2024</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
